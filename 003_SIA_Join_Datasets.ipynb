{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciando o ambiente PySpark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma instância de SparkSession para iniciar uma sessão Spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master('local[*]')\\\n",
    "    .appName('Conversão e manipulação dos arquivos CSV para Parquet')\\\n",
    "    .config('spark.ui.port', '4050')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PYSUS = r'data\\001_PYSUS'\n",
    "PATH_TABNET_UFMUN = r'data\\002_TABNET\\PA_UFMUN.csv'\n",
    "PATH_TABNET_NIVCPL = r'data\\002_TABNET\\PA_NIVCPL.csv'\n",
    "PATH_TABNET_CBOCOD = r'data\\002_TABNET\\PA_CBOCOD.csv'\n",
    "PATH_TABNET_PROC_ID = r'data\\002_TABNET\\PA_PROC_ID.csv'\n",
    "PATH_TABNET_G_PROC_ID = r'data\\002_TABNET\\PA_G_PROC_ID.csv'\n",
    "PATH_TABNET_SG_PROC_ID = r'data\\002_TABNET\\PA_SG_PROC_ID.csv'\n",
    "\n",
    "PATH_PYSUS_PARQUET = r'data\\003_JOIN_DATASETS\\PYSUS_PARQUET'\n",
    "PATH_FINAL_DATASET = r'data\\003_JOIN_DATASETS\\FINAL_DATASET'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando DataFrame Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(path):\n",
    "    return spark.read.csv(path, sep=';', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_pysus = read_csv(PATH_PYSUS)\n",
    "df_pysus.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ufmun = read_csv(PATH_TABNET_UFMUN)\n",
    "df_ufmun.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nivcpl = read_csv(PATH_TABNET_NIVCPL)\n",
    "df_nivcpl.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cbocod = read_csv(PATH_TABNET_CBOCOD)\n",
    "df_cbocod.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc_id = read_csv(PATH_TABNET_PROC_ID)\n",
    "df_proc_id.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g_proc_id = read_csv(PATH_TABNET_G_PROC_ID)\n",
    "df_g_proc_id.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sg_proc_id = read_csv(PATH_TABNET_SG_PROC_ID)\n",
    "df_sg_proc_id.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convertendo base do PySUS de CSV para PARQUET para obter melhor desempenho de manipulação dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 'partitionBy' especifica a coluna pela qual os dados serão particionados. Neste caso, os dados serão particionados pela coluna de data 'PA_CMP'\n",
    "df_pysus.write.parquet(\n",
    "        path=PATH_PYSUS_PARQUET,\n",
    "        mode='overwrite',\n",
    "        partitionBy='PA_CMP' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_pysus = spark.read.parquet(PATH_PYSUS_PARQUET)\n",
    "df_pysus.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'O dataset possui {df_pysus.count()} linhas.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Juntando todos os DataFrames\n",
    "O objetivo é adicionar as colunas de descrição obtidos no TabNet para enriquecer o dataset extraído da biblioteca PySUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_pysus.join(df_ufmun, 'PA_UFMUN', how='left')\n",
    "df.select('PA_UFMUN', 'DS_PA_UFMUN', 'PA_QTDAPR', 'PA_VALAPR')\\\n",
    "  .show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df_nivcpl, 'PA_NIVCPL', how='left')\n",
    "df.select('PA_NIVCPL', 'DS_PA_NIVCPL', 'PA_QTDAPR', 'PA_VALAPR')\\\n",
    "  .show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df_cbocod, 'PA_CBOCOD', how='left')\n",
    "df.select('PA_CBOCOD', 'DS_PA_CBOCOD', 'PA_QTDAPR', 'PA_VALAPR')\\\n",
    "  .show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df_proc_id, 'PA_PROC_ID', how='left')\n",
    "df.select('PA_PROC_ID', 'DS_PA_PROC_ID', 'PA_QTDAPR', 'PA_VALAPR')\\\n",
    "  .show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df_g_proc_id, 'PA_G_PROC_ID', how='left')\n",
    "df.select('PA_G_PROC_ID', 'DS_PA_G_PROC_ID', 'PA_QTDAPR', 'PA_VALAPR')\\\n",
    "  .show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df_sg_proc_id, 'PA_SG_PROC_ID', how='left')\n",
    "df.select('PA_SG_PROC_ID', 'DS_PA_SG_PROC_ID', 'PA_QTDAPR', 'PA_VALAPR')\\\n",
    "  .show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Versão final do Dataset\n",
    "df.write.parquet(\n",
    "        path=PATH_FINAL_DATASET,\n",
    "        mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
